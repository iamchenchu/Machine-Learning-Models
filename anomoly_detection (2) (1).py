# -*- coding: utf-8 -*-
"""Anomoly_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fDtpL3Q-NC48FCWbpOEOEvcZn7d7eOu_
"""

!pip install scapy

import pandas as pd
from scapy.all import rdpcap
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

from google.colab import files
uploaded = files.upload()

# Read the PCAP file (replace 'network_traffic.pcap' with your file)
packets = rdpcap('traffic1.pcap')

# Extract features from packets
data = []
for packet in packets:
    if 'IP' in packet:  # Check if the packet contains an IP layer
        # Extract features from the IP layer
        features = {
            'source_ip': packet['IP'].src,
            'destination_ip': packet['IP'].dst,
            'protocol': packet['IP'].proto,
            'packet_length': len(packet),
            # Add more features as needed
        }
        data.append(features)

# Convert the list of dictionaries to a DataFrame
data = pd.DataFrame(data)

data.dropna(inplace=True)

# Preprocessing: Convert categorical variables to numerical (if needed)
data = pd.get_dummies(data)

print(data.columns)

import numpy as np
from sklearn.metrics import accuracy_score, roc_curve, auc
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_recall_fscore_support
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt

# Data Normalization
scaler = StandardScaler()
data_normalized = scaler.fit_transform(data)

# Data Standardization
# Standardize the data by subtracting the mean and dividing by the standard deviation
scaler = StandardScaler()
data_standardized = scaler.fit_transform(data_normalized)

# True labels (if available)
# Replace this with the actual true labels if available
true_labels = np.array([1] * len(data))

# Initialize StratifiedKFold for cross-validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Lists to store evaluation metrics for each fold
precisions = []
recalls = []
f1_scores = []
accuracies = []  # List to store accuracy for each fold
auc_scores = []  # List to store AUC for each fold

# Loop through each fold
for train_index, test_index in skf.split(data, true_labels):
    # Split data into training and testing sets for the current fold
    X_train, X_test = data.iloc[train_index], data.iloc[test_index]
    y_train, y_test = true_labels[train_index], true_labels[test_index]

    # Train the Isolation Forest model
    model = IsolationForest(contamination=0.05, random_state=42)
    model.fit(X_train)

    # Predict outliers/anomalies
    predictions = model.predict(X_test)

    # Convert predictions to binary labels (1 for normal, -1 for anomaly)
    labels = [1 if pred == 1 else -1 for pred in predictions]

    # Calculate precision, recall, and F1-score for the current fold
    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, labels, average='binary')

    # Calculate accuracy for the current fold
    accuracy = accuracy_score(y_test, labels)

    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(y_test, predictions)

    # Calculate AUC
    auc_score = auc(fpr, tpr)

    # Append the evaluation metrics to the lists
    precisions.append(precision)
    recalls.append(recall)
    f1_scores.append(f1_score)
    accuracies.append(accuracy)
    auc_scores.append(auc_score)

    # Plot ROC curve for the current fold
    plt.plot(fpr, tpr, label=f'ROC Fold {len(auc_scores)} (AUC = {auc_score:.2f})')

# Print average evaluation metrics across all folds
print("Average Precision:", sum(precisions) / len(precisions))
print("Average Recall:", sum(recalls) / len(recalls))
print("Average F1-score:", sum(f1_scores) / len(f1_scores))
print("Average Accuracy:", sum(accuracies) / len(accuracies))
print("Average AUC:", sum(auc_scores) / len(auc_scores))

# Plot ROC curve for all folds
plt.plot([0, 1], [0, 1], linestyle='--', color='black')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt

# Initialize an empty list to store anomaly scores
anomaly_scores = []

# Loop through each fold
for train_index, test_index in skf.split(data, true_labels):
    # Split data into training and testing sets for the current fold
    X_train, X_test = data.iloc[train_index], data.iloc[test_index]

    # Train the Isolation Forest model
    model = IsolationForest(contamination=0.05, random_state=42)
    model.fit(X_train)

    # Obtain anomaly scores for the testing data
    scores = model.decision_function(X_test)

    # Append the anomaly scores to the list
    anomaly_scores.extend(scores)

# Plot histogram of anomaly scores
plt.figure(figsize=(10, 6))
plt.hist(anomaly_scores, bins=50, density=True, alpha=0.7, color='blue')
plt.xlabel('Anomaly Score')
plt.ylabel('Density')
plt.title('Histogram of Anomaly Scores')
plt.show()

# Train the Isolation Forest model
model = IsolationForest(contamination=0.05, random_state=42)
model.fit(data)

# Predict outliers/anomalies
predictions = model.predict(data)

# Convert predictions to binary labels (1 for normal, -1 for anomaly)
labels = [1 if pred == 1 else -1 for pred in predictions]

# Output labels for evaluation or further analysis
print(labels)

from sklearn.metrics import precision_recall_fscore_support

# Calculate precision, recall, f1-score, and support for each class
precision, recall, f1_score, support = precision_recall_fscore_support(true_labels, labels)

# Print the results
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1_score)
print("Support:", support)

# True labels (if available)
# Replace this with the actual true labels if available
true_labels = [1] * len(labels)

# Calculate and print the classification report
print(classification_report(true_labels, labels))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Plot histogram of anomaly scores
plt.figure(figsize=(10, 6))
plt.hist(model.decision_function(data), bins=50, density=True, alpha=0.7, color='blue')
plt.xlabel('Anomaly Score')
plt.ylabel('Density')
plt.title('Histogram of Anomaly Scores')
plt.show()

# Scatter plot of features (example with packet length vs. protocol)
plt.figure(figsize=(10, 6))
sns.scatterplot(x='packet_length', y='protocol', hue=labels, data=data)
plt.xlabel('Packet Length')
plt.ylabel('Protocol')
plt.title('Scatter Plot of Packet Length vs. Protocol')
plt.show()

# Confusion matrix
conf_matrix = confusion_matrix(true_labels, labels)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()